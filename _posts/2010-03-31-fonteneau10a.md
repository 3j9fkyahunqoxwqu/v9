---
title: Model-Free Monte Carlo-like Policy Evaluation
abstract: We propose an algorithm for estimating the finite-horizon expected return
  of a closed loop control policy from an a priori given (off-policy) sample of one-step
  transitions. It averages cumulated rewards along a set of “broken trajectories”
  made of one-step transitions selected from the sample on the basis of the control
  policy. Under some Lipschitz continuity assumptions on the system dynamics, reward
  function and control policy, we provide bounds on the bias and variance of the estimator
  that depend only on the Lipschitz constants, on the number of broken trajectories
  used in the estimator, and on the  sparsity of the sample of one-step transitions.
pdf: http://proceedings.mlr.press/v9/fonteneau10a/fonteneau10a.pdf
layout: inproceedings
series: Proceedings of Machine Learning Research
id: fonteneau10a
month: 0
tex_title: Model-Free Monte Carlo-like Policy Evaluation
firstpage: 217
lastpage: 224
page: 217-224
order: 217
cycles: false
author:
- given: Raphael
  family: Fonteneau
- given: Susan
  family: Murphy
- given: Louis
  family: Wehenkel
- given: Damien
  family: Ernst
date: 2010-03-31
address: Chia Laguna Resort, Sardinia, Italy
publisher: PMLR
container-title: Proceedings of the Thirteenth International Conference on Artificial
  Intelligence and Statistics
volume: '9'
genre: inproceedings
issued:
  date-parts:
  - 2010
  - 3
  - 31
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
