---
title: Regret Bounds for Gaussian Process Bandit Problems
abstract: Bandit algorithms are concerned with trading exploration with exploitation
  where a number of options are available but we can only learn their quality by experimenting
  with them. We consider the scenario in which the reward distribution for arms is
  modeled by a Gaussian process and there is no noise in the observed reward. Our
  main result is to bound the regret experienced by algorithms relative to the a posteriori
  optimal strategy of playing the best arm throughout based on benign assumptions
  about the covariance function defining the Gaussian process. We further complement
  these upper bounds with corresponding lower bounds for particular covariance functions
  demonstrating that in general there is at most a logarithmic looseness in our upper
  bounds.
pdf: http://proceedings.mlr.press/v9/grunewalder10a/grunewalder10a.pdf
layout: inproceedings
series: Proceedings of Machine Learning Research
id: grunewalder10a
month: 0
firstpage: '273'
lastpage: '280'
page: 273-280
sections: 
author:
- given: Steffen
  family: Grünewälder
- given: Jean–Yves
  family: Audibert
- given: Manfred
  family: Opper
- given: John
  family: Shawe–Taylor
date: 2010-03-31
address: Chia Laguna Resort, Sardinia, Italy
publisher: PMLR
container-title: Proceedings of the Thirteenth International Conference on Artificial
  Intelligence and Statistics
volume: '9'
genre: inproceedings
issued:
  date-parts:
  - 2010
  - 3
  - 31
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
