---
title: Why Does Unsupervised Pre-training Help Deep Learning?
abstract: 'Much recent research has been devoted to learning algorithms for deep architectures
  such as Deep Belief Networks and stacks of auto-encoder   variants with impressive
  results being obtained in several areas, mostly   on vision and language datasets.  The
  best results obtained on supervised   learning tasks often involve an unsupervised
  learning component, usually   in an unsupervised pre-training phase. The main question
  investigated   here is the following: why does unsupervised pre-training work so
  well?   Through extensive experimentation, we explore several possible   explanations
  discussed in the literature including its action as a   regularizer (Erhan et al.
  2009) and as an aid to optimization   (Bengio et al. 2007).  Our results build on
  the work of   Erhan et al. 2009, showing that unsupervised pre-training appears
  to   play predominantly a regularization role in subsequent supervised   training.
  However our results in an online setting, with a virtually unlimited   data stream,
  point to a somewhat more nuanced interpretation of the roles   of optimization and
  regularization in the unsupervised pre-training   effect.'
pdf: http://jmlr.org/proceedings/papers/v9/erhan10a/erhan10a.pdf
layout: inproceedings
id: erhan10a
month: 0
firstpage: 201
lastpage: 208
page: 201-208
sections: 
author:
- given: Dumitru
  family: Erhan
- given: Aaron
  family: Courville
- given: Yoshua
  family: Bengio
- given: Pascal
  family: Vincent
reponame: v9
date: 2010-03-31
address: Chia Laguna Resort, Sardinia, Italy
publisher: PMLR
container-title: Proceedings of the Thirteenth International Conference on Artificial
  Intelligence and Statistics
volume: '9'
genre: inproceedings
issued:
  date-parts:
  - 2010
  - 3
  - 31
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
