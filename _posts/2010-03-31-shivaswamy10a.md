---
title: Empirical Bernstein Boosting
abstract: Concentration inequalities that incorporate variance information (such as
  Bernstein’s or Bennett’s inequality) are often significantly tighter than counterparts
  (such as Hoeffding’s inequality) that disregard variance. Nevertheless, many state
  of the art machine learning algorithms for classification problems like AdaBoost
  and support vector machines (SVMs) extensively use Hoeffding’s inequalities to justify
  empirical risk minimization and its variants. This article proposes a novel boosting
  algorithm based on a recently introduced principle–sample variance penalization–which
  is motivated from an empirical version of Bernstein’s inequality.  This framework
  leads to an efficient algorithm that is as easy to implement as AdaBoost while producing
  a strict generalization. Experiments on a large number of datasets show significant
  performance gains over AdaBoost. This paper shows that sample variance penalization
  could be a viable alternative to empirical risk minimization.
pdf: http://proceedings.mlr.press/v9/shivaswamy10a/shivaswamy10a.pdf
layout: inproceedings
series: Proceedings of Machine Learning Research
id: shivaswamy10a
month: 0
tex_title: Empirical Bernstein Boosting
firstpage: 733
lastpage: 740
page: 733-740
sections: 
author:
- given: Pannagadatta
  family: Shivaswamy
- given: Tony
  family: Jebara
date: 2010-03-31
address: Chia Laguna Resort, Sardinia, Italy
publisher: PMLR
container-title: Proceedings of the Thirteenth International Conference on Artificial
  Intelligence and Statistics
volume: '9'
genre: inproceedings
issued:
  date-parts:
  - 2010
  - 3
  - 31
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
