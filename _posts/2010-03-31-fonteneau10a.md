---
title: Model-Free Monte Carlo-like Policy Evaluation
abstract: We propose an algorithm for estimating the finite-horizon expected return
  of a closed loop control policy from an a priori given (off-policy) sample of one-step
  transitions. It averages cumulated rewards along a set of ``broken trajectories''
  made of one-step transitions selected from the sample on the basis of the control
  policy. Under some Lipschitz continuity assumptions on the system dynamics, reward
  function and control policy, we provide bounds on the bias and variance of the estimator
  that depend only on the Lipschitz constants, on the number of broken trajectories
  used in the estimator, and on the  sparsity of the sample of one-step transitions.
pdf: "./fonteneau10a/fonteneau10a.pdf"
layout: inproceedings
id: fonteneau10a
month: 0
firstpage: 217
lastpage: 224
page: 217-224
origpdf: http://jmlr.org/proceedings/papers/v9/fonteneau10a/fonteneau10a.pdf
sections: 
author:
- given: Raphael
  family: Fonteneau
- given: Susan
  family: Murphy
- given: Louis
  family: Wehenkel
- given: Damien
  family: Ernst
date: '2010-03-31 00:03:37'
publisher: PMLR
---
