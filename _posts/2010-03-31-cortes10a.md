---
title: On the Impact of Kernel Approximation on Learning Accuracy
abstract: Kernel approximation is commonly used to scale kernel-based algorithms to
  applications containing as many as several million instances. This paper analyzes
  the effect of such approximations in the kernel matrix on the hypothesis generated
  by several widely used learning algorithms. We give stability bounds based on the
  norm of the kernel approximation for these algorithms, including SVMs, KRR, and
  graph Laplacian-based regularization algorithms. These bounds help determine the
  degree of approximation that can be tolerated in the estimation of the kernel matrix.
  Our analysis is general and applies to arbitrary approximations of the kernel matrix.
  However, we also give a specific analysis of the Nystrom low-rank approximation
  in this context and report the results of experiments evaluating the quality of
  the Nystrom low-rank kernel approximation when used with ridge regression.
pdf: "./cortes10a/cortes10a.pdf"
layout: inproceedings
id: cortes10a
month: 0
firstpage: 113
lastpage: 120
page: 113-120
origpdf: http://jmlr.org/proceedings/papers/v9/cortes10a/cortes10a.pdf
sections: 
author:
- given: Corinna
  family: Cortes
- given: Mehryar
  family: Mohri
- given: Ameet
  family: Talwalkar
date: '2010-03-31 00:01:53'
publisher: PMLR
---
