---
title: Regret Bounds for Gaussian Process Bandit Problems
abstract: Bandit algorithms are concerned with trading exploration with exploitation
  where a number of options are available but we can only learn their quality by experimenting
  with them. We consider the scenario in which the reward distribution for arms is
  modeled by a Gaussian process and there is no noise in the observed reward. Our
  main result is to bound the regret experienced by algorithms relative to the a posteriori
  optimal strategy of playing the best arm throughout based on benign assumptions
  about the covariance function defining the Gaussian process. We further complement
  these upper bounds with corresponding lower bounds for particular covariance functions
  demonstrating that in general there is at most a logarithmic looseness in our upper
  bounds.
pdf: "./grunewalder10a/grunewalder10a.pdf"
layout: inproceedings
id: grunewalder10a
month: 0
firstpage: 273
lastpage: 280
page: 273-280
origpdf: http://jmlr.org/proceedings/papers/v9/grunewalder10a/grunewalder10a.pdf
sections: 
author:
- given: Steffen
  family: Grünewälder
- given: Jean–Yves
  family: Audibert
- given: Manfred
  family: Opper
- given: John
  family: Shawe–Taylor
date: '2010-03-31 00:04:33'
publisher: PMLR
---
