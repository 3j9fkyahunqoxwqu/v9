---
title: Understanding the difficulty of training deep feedforward neural networks
abstract: Whereas before 2006 it appears that deep multi-layer neural networks were
  not successfully trained, since then several algorithms have been shown to successfully
  train them, with experimental results showing the superiority of deeper vs less
  deep architectures. All these experimental results were obtained with new initialization
  or training mechanisms. Our objective here is to understand better why standard
  gradient descent from random initialization is doing so poorly with deep neural
  networks, to better understand these recent relative successes and help design better
  algorithms in the future.  We first observe the influence of the non-linear activations
  functions. We find that the logistic sigmoid activation is unsuited for deep networks
  with random initialization because of its mean value, which can drive especially
  the top hidden layer into saturation. Surprisingly, we find that saturated units
  can move out of saturation by themselves, albeit slowly, and explaining the plateaus
  sometimes seen when training neural networks. We find that a new non-linearity that
  saturates less can often be beneficial. Finally, we study how activations and gradients
  vary across layers and during training, with the idea that training may be more
  difficult when the singular values of the Jacobian associated with each layer are
  far from 1.  Based on these considerations, we propose a new initialization scheme
  that brings substantially faster convergence.
pdf: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf
layout: inproceedings
id: glorot10a
month: 0
firstpage: 249
lastpage: 256
page: 249-256
sections: 
author:
- given: Xavier
  family: Glorot
- given: Yoshua
  family: Bengio
date: 2010-03-31
address: Chia Laguna Resort, Sardinia, Italy
publisher: PMLR
container-title: Proceedings of the Thirteenth International Conference on Artificial
  Intelligence and Statistics
volume: '9'
genre: inproceedings
issued:
  date-parts:
  - 2010
  - 3
  - 31
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
