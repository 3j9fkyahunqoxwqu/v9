---
title: Kernel Partial Least Squares is Universally Consistent
abstract: We prove the statistical consistency of kernel Partial Least Squares Regression
  applied to a bounded regression learning problem on a reproducing kernel Hilbert
  space. Partial Least Squares  stands out of well-known classical approaches as e.g.
  Ridge Regression or Principal Components Regression, as it is not defined as the
  solution of a global cost minimization procedure over a fixed model nor is it a
  linear estimator. Instead, approximate solutions are constructed by projections
  onto a nested set of data-dependent subspaces. To prove consistency, we exploit
  the known fact that Partial Least Squares is equivalent to the conjugate gradient
  algorithm in combination with early stopping. The choice of the stopping rule (number
  of iterations) is a crucial point. We study two empirical stopping rules. The first
  one monitors the estimation error in each iteration step of Partial Least Squares,
  and the second one estimates the empirical complexity in terms of a condition number.
  Both stopping rules lead to universally consistent estimators provided the kernel
  is universal.
pdf: http://proceedings.mlr.press/v9/blanchard10a/blanchard10a.pdf
layout: inproceedings
series: Proceedings of Machine Learning Research
id: blanchard10a
month: 0
tex_title: Kernel Partial Least Squares is Universally Consistent
firstpage: 57
lastpage: 64
page: 57-64
order: 57
cycles: false
author:
- given: Gilles
  family: Blanchard
- given: Nicole
  family: Kr√§mer
date: 2010-03-31
address: Chia Laguna Resort, Sardinia, Italy
publisher: PMLR
container-title: Proceedings of the Thirteenth International Conference on Artificial
  Intelligence and Statistics
volume: '9'
genre: inproceedings
issued:
  date-parts:
  - 2010
  - 3
  - 31
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
